[
    {
        "id": "e7074517-c784-481a-aeed-74e01amemodo",
        "userId": "ec180714-3bca-4710-a7cd-4e2e8975daac",
        "function": {
            "id": "memodo_rag_pipe",
            "name": "Memodo RAG Pipe Function",
            "meta": {
                "description": "Memodo Custom RAG",
                "type": "pipe",
                "manifest": {
                    "title": "Memodo RAG Pipe Function",
                    "author": "Memodo GmbH (mailto:p.oliva@memodo.de)",
                    "author_url": "https://github.com/memodo",
                    "version": "0.1.0"
                }
            },
            "content": "\"\"\"\r\ntitle: Memodo RAG Pipe-Function\r\nauthor: Memodo GmbH (mailto:p.oliva@memodo.de)\r\nauthor_url: https:\/\/github.com\/memodo\r\nversion: 0.1.0\r\ndescription: This module defines a Pipe class that will be used as a Function in Open WebUI.\r\nrequirements: langchain-ollama\r\n\"\"\"\r\n\r\nfrom typing import List, Optional, Callable, Awaitable\r\nfrom pydantic import BaseModel, ConfigDict, Field\r\nimport time\r\nimport chromadb\r\n\r\nfrom langchain.schema import StrOutputParser\r\nfrom langchain_core.runnables import RunnablePassthrough\r\nfrom langchain_ollama import OllamaLLM, OllamaEmbeddings\r\nfrom langchain_core.output_parsers import StrOutputParser\r\nfrom langchain.prompts import PromptTemplate\r\n\r\nfrom chromadb.config import Settings\r\nfrom pydantic import BaseModel\r\n\r\nclass Pipe:\r\n    class Valves(BaseModel):\r\n        model_config = ConfigDict(arbitrary_types_allowed=True)\r\n\r\n        collection_name: str = Field(default=\"product_management\", description=\"The name of the collection used in the ChromaDB database. The collection name must be between 3 and 63 characters long, start and end with a lowercase letter or a digit, and can only include dots, dashes, and underscores in between.\")\r\n        ollama_host: str = Field(default=\"host.docker.internal\")\r\n        ollama_port: str = Field(default=\"11434\")\r\n        chroma_host: str = Field(default=\"host.docker.internal\")\r\n        chroma_port: str = Field(default=\"8000\")\r\n        embeddingModel: str = Field(default=\"paraphrase-multilingual\")\r\n        chatModel: str = Field(default=\"llama3.2:3b\")\r\n        emit_interval: float = Field(\r\n            default=2.0, description=\"Interval in seconds between status emissions\"\r\n        )\r\n        enable_status_indicator: bool = Field(\r\n            default=True, description=\"Enable or disable status indicator emissions\"\r\n        )\r\n\r\n    def __init__(self):\r\n        self.type = \"pipe\"\r\n        self.id = \"memodo_rag_pipe\"\r\n        self.name = \"Memodo RAG Pipe\"\r\n        self.valves = self.Valves()\r\n        self.last_emit_time = 0\r\n        pass\r\n\r\n    async def emit_status(\r\n        self,\r\n        __event_emitter__: Callable[[dict], Awaitable[None]],\r\n        level: str,\r\n        message: str,\r\n        done: bool,\r\n    ):\r\n        current_time = time.time()\r\n        if (\r\n            __event_emitter__\r\n            and self.valves.enable_status_indicator\r\n            and (\r\n                current_time - self.last_emit_time >= self.valves.emit_interval or done\r\n            )\r\n        ):\r\n            await __event_emitter__(\r\n                {\r\n                    \"type\": \"status\",\r\n                    \"data\": {\r\n                        \"status\": \"complete\" if done else \"in_progress\",\r\n                        \"level\": level,\r\n                        \"description\": message,\r\n                        \"done\": done,\r\n                    },\r\n                }\r\n            )\r\n            self.last_emit_time = current_time\r\n\r\n    async def pipe(self, \r\n        body: dict,\r\n        __user__: Optional[dict] = None,\r\n        __event_emitter__: Callable[[dict], Awaitable[None]] = None,\r\n        __event_call__: Callable[[dict], Awaitable[dict]] = None,\r\n    ) -> Optional[dict]:\r\n        await self.emit_status(\r\n            __event_emitter__, \"info\", \"\/initiating Chain\", False\r\n        )\r\n\r\n        self.model_instance = OllamaLLM(model=self.valves.chatModel, base_url=f\"http:\/\/{self.valves.ollama_host}:{self.valves.ollama_port}\")\r\n        # embeddings = OllamaEmbeddings(model=self.valves.chatModel, base_url=f\"http:\/\/{self.valves.ollama_host}:{self.valves.ollama_port}\")\r\n\r\n        client = chromadb.HttpClient(\r\n            self.valves.chroma_host,\r\n            self.valves.chroma_port,\r\n            settings=Settings(\r\n                chroma_client_auth_provider=\"chromadb.auth.token_authn.TokenAuthClientProvider\",\r\n                chroma_client_auth_credentials=\"chromadb-test-token\",\r\n                chroma_auth_token_transport_header=\"Authorization\"\r\n            )\r\n        )\r\n        self.vector_db_documents = client.get_collection(name=self.valves.collection_name)\r\n\r\n        multiquery_template = \"\"\"\r\n        You are an AI language model assistant.\r\n\r\n        Your task is to generate two different versions of the given user question to retrieve relevant documents from a vector database. The two versions should be different from each other and in the same language as the user question.\r\n\r\n        By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of distance-based similarity search.\r\n\r\n        Provide these alternative questions separated by newlines. Include the original question as well. Only provide the questions, no other text.\r\n\r\n        Original question: {question}\r\n        \"\"\"\r\n        \r\n        rag_template = \"\"\"\r\n        You are given a user query, some textual context, rules and steps, all inside xml tags. You have to answer the query based on the context while respecting the rules.\r\n\r\n        <context>\r\n        {context}\r\n        <\/context>\r\n\r\n        <rules>\r\n        - If you don't know, just say so.\r\n        - If you are not sure, ask for clarification.\r\n        - Answer in the same language as the user query.\r\n        - If the context appears unreadable or of poor quality, tell the user then answer as best as you can.\r\n        - If the answer is not in the context but you think you know the answer, explain that to the user then answer with your own knowledge.\r\n        - Answer directly and without using xml tags.\r\n        <\/rules>\r\n\r\n        <steps>\r\n        Step 1: Parse Context Information\r\n        Extract and utilize relevant knowledge from the provided context within `<context><\/context>` XML tags.  \r\n        Step 2: Analyze User Query\r\n        Carefully read and comprehend the user's query, pinpointing the key concepts, entities, and intent behind the question.  \r\n        Step 3: Determine Response\r\n        If the answer to the user's query can be directly inferred from the context information, provide a concise and accurate response in the same language as the user's query.  \r\n        Step 4: Handle Uncertainty\r\n        If the answer is not clear, ask the user for clarification to ensure an accurate response.  \r\n        Step 5: Avoid Context Attribution\r\n        When formulating your response, do not indicate that the information was derived from the context.  \r\n        Step 6: Respond in User's Language\r\n        Maintain consistency by ensuring the response is in the same language as the user's query.  \r\n        Step 7: Provide Response\r\n        Generate a clear, concise, and informative response to the user's query, adhering to the guidelines outlined above.  \r\n        <\/steps>\r\n\r\n        <user_query>\r\n        {question}\r\n        <\/user_query>\r\n        \"\"\"\r\n\r\n        multiquery_prompt = PromptTemplate.from_template(multiquery_template)\r\n        rag_prompt = PromptTemplate.from_template(rag_template)\r\n\r\n        chain = (\r\n            { \"question\": RunnablePassthrough() }\r\n            | multiquery_prompt\r\n            | self.model_instance\r\n            | self.lineListOutputParser\r\n            | {\r\n                \"context\": lambda x: self.query_db(x),\r\n                \"question\": lambda _: question\r\n            }\r\n            | rag_prompt\r\n            | self.model_instance\r\n            | StrOutputParser()\r\n        )\r\n\r\n        await self.emit_status(\r\n            __event_emitter__, \"info\", \"Starting Memodo RAG Chain\", False\r\n        )\r\n\r\n        messages = body.get(\"messages\", [])\r\n        \r\n        if messages:\r\n            question = messages[-1][\"content\"]\r\n            try:\r\n                response = chain.invoke(question)\r\n                # Set assitant message with chain reply\r\n                body[\"messages\"].append({\"role\": \"assistant\", \"content\": response})\r\n            except Exception as e:\r\n                await self.emit_status(__event_emitter__, \"error\", f\"Error during Memodo RAG execution: {str(e)}\", True)\r\n                return {\"error\": str(e)}\r\n        else:\r\n            await self.emit_status(__event_emitter__, \"error\", \"No messages found in the request body\", True)\r\n            body[\"messages\"].append({\"role\": \"assistant\", \"content\": \"No messages found in the request body\"})\r\n\r\n        await self.emit_status(__event_emitter__, \"info\", \"Memodo RAG Chain complete\", True)\r\n        return response\r\n    \r\n    def lineListOutputParser(self, text: str) -> List[str]:\r\n        lines = text.strip().split(\"\\n\")\r\n        return lines  \r\n\r\n    def query_db(self, questions):\r\n        results = self.vector_db_documents.query(\r\n            query_texts=questions,\r\n            n_results=2\r\n        )\r\n        return results[\"documents\"]"
        },
        "downloads": 0,
        "upvotes": 0,
        "downvotes": 0,
        "updatedAt": 1733301414,
        "createdAt": 1733301414,
        "user": {
            "id": "ec180714-3bca-4710-a7cd-4e2e8975daac",
            "username": "pomemodo",
            "name": "",
            "profileImageUrl": "",
            "createdAt": 1733301414
        }
    }
]